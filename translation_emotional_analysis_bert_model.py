# -*- coding: utf-8 -*-
"""Translation_Emotional_Analysis_BERT_Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wdvZQb6EblmlVBcEsXbHe7pvhOZNWr5B
"""

!pip install transformers

from google.colab import files
from imblearn.under_sampling import RandomUnderSampler
import pandas as pd
import tensorflow as tf
import numpy as np
from transformers import *
from tqdm import tqdm
import os
import tqdm
import io
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import classification_report, confusion_matrix
import torch.nn as nn
from torch.optim import AdamW
from torch.utils.data import Dataset, DataLoader
import warnings
import torch
from gensim.utils import simple_preprocess
import seaborn as sns
import matplotlib.pyplot as plt
warnings.filterwarnings("ignore")

if torch.cuda.is_available():       
    device = torch.device("cuda")
    print(f'There are {torch.cuda.device_count()} GPU(s) available.')
    print('Device name:', torch.cuda.get_device_name(0))

else:
    print('No GPU available, using the CPU instead.')
    device = torch.device("cpu")

EPOCHS = 6
N_SPLITS = 5

#uploads = files.upload()
#uploads = files.upload()
#uploads = files.upload()
#uploads = files.upload()
#uploads = files.upload()
#uploads = files.upload()
def get_meld_data(path):
  df = pd.read_csv(path)
  df.columns = ['No.','Sentence','Speaker','Emotion','Sentiment','Dialogue_ID','Utterance_ID','Season','Episode','StartTime','Endtime']
  df.drop(columns = ['No.','Speaker','Sentiment','Dialogue_ID','Utterance_ID','Season','Episode','StartTime','Endtime'], inplace=True)
  return df
meld_test_df = get_meld_data('test_sent_emo.csv')
meld_train_df = get_meld_data('train_sent_emo.csv')
meld_valid_df = get_meld_data('dev_sent_emo.csv')

#Merge train and validation set
meld_train_df = pd.concat([meld_train_df, meld_valid_df], ignore_index=True) #Add caption to graph for clarification, what it means and insight
sns.countplot(x='Emotion', data=meld_train_df)

meld_train_df.Emotion.value_counts()

meld_test_df.info(), meld_train_df.info()

meld_train_df.sample(10)

#Undersampling the majority emotion, which is neutral for a more balanced dataset
#sampl_dict = {'neutral':2000}
#rus = RandomUnderSampler(random_state=11, sampling_strategy=sampl_dict)

#use k-fold cross validation

#meld_train_df, meld_train_df.Emotion[''] = rus.fit_resample(meld_train_df,meld_train_df.Emotion)
skf = StratifiedKFold(n_splits=N_SPLITS)
for fold, (_, val_) in enumerate(skf.split(X=meld_train_df, y=meld_train_df.Emotion)):
    meld_train_df.loc[val_, "kfold"] = fold

meld_train_df.sample(10)

meld_train_df.Emotion.value_counts()

sns.countplot(x='Emotion', data=meld_train_df) #caption needed here

def get_data(path):
    df = pd.read_excel(path, sheet_name=None)['Sheet1']
    df.columns = ['index', 'Emotion', 'Sentence']
    # unused column
    df.drop(columns=['index'], inplace=True)
    return df

vsmec_train_df = get_data('train_nor_811.xlsx')
vsmec_test_df = get_data('test_nor_811.xlsx')
vsmec_valid_df = get_data('valid_nor_811.xlsx')

vsmec_train_df = pd.concat([vsmec_train_df, vsmec_valid_df], ignore_index=True)
skf = StratifiedKFold(n_splits=N_SPLITS)
for fold, (_, val_) in enumerate(skf.split(X=vsmec_train_df, y=vsmec_train_df.Emotion)):
    vsmec_train_df.loc[val_, "kfold"] = fold

vsmec_train_df.info(), vsmec_test_df.info()

vsmec_train_df.sample(10)

sns.countplot(x='Emotion', data=vsmec_train_df)

VNtokenizer = AutoTokenizer.from_pretrained("vinai/phobert-base", use_fast=False)

class SentimentDataset(Dataset):
    def __init__(self, df, tokenizer, max_len=120):
        self.df = df
        self.max_len = max_len
        self.tokenizer = tokenizer
    
    def __len__(self):
        return len(self.df)

    def __getitem__(self, index):
        """
        To customize dataset, inherit from Dataset class and implement
        __len__ & __getitem__
        __getitem__ should return 
            data:
                input_ids
                attention_masks
                text
                targets
        """
        row = self.df.iloc[index]
        text, label = self.get_input_data(row)

        # Encode_plus will:
        # (1) split text into token
        # (2) Add the '[CLS]' and '[SEP]' token to the start and end
        # (3) Truncate/Pad sentence to max length
        # (4) Map token to their IDS
        # (5) Create attention mask
        # (6) Return a dictionary of outputs
        encoding = self.tokenizer.encode_plus(
            text,
            truncation=True,
            add_special_tokens=True,
            max_length=self.max_len,
            padding='max_length',
            return_attention_mask=True,
            return_token_type_ids=False,
            return_tensors='pt',
        )
        
        return {
            'text': text,
            'input_ids': encoding['input_ids'].flatten(),
            'attention_masks': encoding['attention_mask'].flatten(),
            'targets': torch.tensor(label, dtype=torch.long),
        }


    def labelencoder(self,text):
        if text.lower()=='enjoyment' or text.lower()=="joy":
            return 0
        elif text.lower()=='disgust':
            return 1
        elif text.lower()=='sadness':
            return 2
        elif text.lower()=='anger':
            return 3
        elif text.lower()=='surprise':
            return 4
        elif text.lower()=='fear':
            return 5
        else:
            return 6

    def get_input_data(self, row):
        # Preprocessing: {remove icon, special character, lower}
        text = row['Sentence']
        text = ' '.join(simple_preprocess(text))
        label = self.labelencoder(row['Emotion'])

        return text, label

class VNSentimentClassifier(nn.Module):
    def __init__(self, n_classes):
        super(VNSentimentClassifier, self).__init__()
        self.bert = AutoModel.from_pretrained("vinai/phobert-base")
        self.drop = nn.Dropout(p=0.3)
        self.fc = nn.Linear(self.bert.config.hidden_size, n_classes)
        nn.init.normal_(self.fc.weight, std=0.02)
        nn.init.normal_(self.fc.bias, 0)

    def forward(self, input_ids, attention_mask):
        last_hidden_state, output = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            return_dict=False # Dropout will errors if without this
        )

        x = self.drop(output)
        x = self.fc(x)
        return x

def train(model, criterion, optimizer, train_loader):
    model.train()
    losses = []
    correct = 0

    for data in train_loader:
        input_ids = data['input_ids'].to(device)
        attention_mask = data['attention_masks'].to(device)
        targets = data['targets'].to(device)

        optimizer.zero_grad()
        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask
        )

        loss = criterion(outputs, targets)
        _, pred = torch.max(outputs, dim=1)

        correct += torch.sum(pred == targets)
        losses.append(loss.item())
        loss.backward()
        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        lr_scheduler.step()

    print(f'Train Accuracy: {correct.double()/len(train_loader.dataset)} Loss: {np.mean(losses)}')

def eval(model,test_data = False):
    model.eval()
    losses = []
    correct = 0

    with torch.no_grad():
        data_loader = test_loader if test_data else valid_loader
        for data in data_loader:
            input_ids = data['input_ids'].to(device)
            attention_mask = data['attention_masks'].to(device)
            targets = data['targets'].to(device)

            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask
            )

            _, pred = torch.max(outputs, dim=1)

            loss = criterion(outputs, targets)
            correct += torch.sum(pred == targets)
            losses.append(loss.item())
    
    if test_data:
        print(f'Test Accuracy: {correct.double()/len(test_loader.dataset)} Loss: {np.mean(losses)}')
        return correct.double()/len(test_loader.dataset)
    else:
        print(f'Valid Accuracy: {correct.double()/len(valid_loader.dataset)} Loss: {np.mean(losses)}')
        return correct.double()/len(valid_loader.dataset)

def prepare_loaders(df, fold, tokenizer):
    df_train = df[df.kfold != fold].reset_index(drop=True)
    df_valid = df[df.kfold == fold].reset_index(drop=True)
    
    train_dataset = SentimentDataset(df_train, tokenizer, max_len=120)
    valid_dataset = SentimentDataset(df_valid, tokenizer, max_len=120)
    
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)
    valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=True, num_workers=2)
    
    return train_loader, valid_loader

for fold in range(skf.n_splits):
    print(f'-----------Fold: {fold+1} ------------------')
    train_loader, valid_loader = prepare_loaders(vsmec_train_df, fold=fold,tokenizer=VNtokenizer)
    model = VNSentimentClassifier(n_classes=7).to(device)
    criterion = nn.CrossEntropyLoss()
    # Recommendation by BERT: lr: 5e-5, 2e-5, 3e-5
    # Batchsize: 16, 32
    optimizer = AdamW(model.parameters(), lr=2e-5)
    
    lr_scheduler = get_linear_schedule_with_warmup(
                optimizer, 
                num_warmup_steps=0, 
                num_training_steps=len(train_loader)*EPOCHS
            )
    best_acc = 0
    for epoch in range(EPOCHS):
        print(f'Epoch {epoch+1}/{EPOCHS}')
        print('-'*30)

        train(model, criterion, optimizer, train_loader)
        val_acc = eval(model)

        if val_acc > best_acc:
            torch.save(model.state_dict(), f'phobert_fold{fold+1}.pth')
            best_acc = val_acc

def VNtest(data_loader):
    models = []
    for fold in range(skf.n_splits):
        model = VNSentimentClassifier(n_classes=7)
        model.to(device)
        model.load_state_dict(torch.load(f'phobert_fold{fold+1}.pth'))
        model.eval()
        models.append(model)

    texts = []
    predicts = []
    predict_probs = []
    real_values = []

    for data in data_loader:
        text = data['text']
        input_ids = data['input_ids'].to(device)
        attention_mask = data['attention_masks'].to(device)
        targets = data['targets'].to(device)

        total_outs = []
        for model in models:
            with torch.no_grad():
                outputs = model(
                    input_ids=input_ids,
                    attention_mask=attention_mask
                )
                total_outs.append(outputs)
        
        total_outs = torch.stack(total_outs)
        _, pred = torch.max(total_outs.mean(0), dim=1)
        texts.extend(text)
        predicts.extend(pred)
        predict_probs.extend(total_outs.mean(0))
        real_values.extend(targets)
    
    predicts = torch.stack(predicts).cpu()
    predict_probs = torch.stack(predict_probs).cpu()
    real_values = torch.stack(real_values).cpu()
    print(classification_report(real_values, predicts))
    return real_values, predicts

test_dataset = SentimentDataset(vsmec_test_df, VNtokenizer, max_len=120)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True, num_workers=2)
real_values, predicts = VNtest(test_loader)

class_names = ['Enjoyment', 'Disgust', 'Sadness', 'Anger', 'Surprise', 'Fear', 'Other']
sns.heatmap(confusion_matrix(real_values, predicts, normalize='true'), annot=True, xticklabels = class_names, yticklabels = class_names)

def check_wrong(real_values, predicts):
    wrong_arr = []
    wrong_label = []
    for i in range(len(predicts)):
        if predicts[i] != real_values[i]:
            wrong_arr.append(i)
            wrong_label.append(predicts[i])
    return wrong_arr, wrong_label

for i in range(15):
    print('-'*50)
    wrong_arr, wrong_label = check_wrong(real_values, predicts)
    print(vsmec_test_df.iloc[wrong_arr[i]].Sentence)
    print(f'Predicted: ({class_names[wrong_label[i]]}) --vs-- Real label: ({class_names[real_values[wrong_arr[i]]]})')

def infer(text,model, tokenizer, max_len=120):
    encoded_review = tokenizer.encode_plus(
        text,
        max_length=max_len,
        truncation=True,
        add_special_tokens=True,
        padding='max_length',
        return_attention_mask=True,
        return_token_type_ids=False,
        return_tensors='pt',
    )

    input_ids = encoded_review['input_ids'].to(device)
    attention_mask = encoded_review['attention_mask'].to(device)

    output = model(input_ids, attention_mask)
    _, y_pred = torch.max(output, dim=1)

    print(f'Text: {text}')
    print(f'Sentiment: {class_names[y_pred]}')

infer('Tôi rất vui khi được gặp bạn',model, VNtokenizer)

ENtokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

class ENSentimentClassifier(nn.Module):
    """Bert Model for Classification Tasks.
    """
    def __init__(self, n_classes, freeze_bert=False):
        """
        @param    bert: a BertModel object
        @param    classifier: a torch.nn.Module classifier
        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model
        """
        super(ENSentimentClassifier, self).__init__()

        # Instantiate BERT model
        self.bert = AutoModel.from_pretrained('bert-base-uncased')

        # Instantiate an one-layer feed-forward classifier
        self.drop = nn.Dropout(p=0.3)
        self.fc = nn.Linear(self.bert.config.hidden_size, n_classes)
        nn.init.normal_(self.fc.weight, std=0.02)
        nn.init.normal_(self.fc.bias, 0)

        # Freeze the BERT model
        if freeze_bert:
            for param in self.bert.parameters():
                param.requires_grad = False
        
    def forward(self, input_ids, attention_mask):
        """
        Feed input to BERT and the classifier to compute logits.
        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,
                      max_length)
        @param    attention_mask (torch.Tensor): a tensor that hold attention mask
                      information with shape (batch_size, max_length)
        @return   logits (torch.Tensor): an output tensor with shape (batch_size,
                      num_labels)
        """
        # Feed input to BERT
        last_hidden_state, output = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            return_dict=False # Dropout will errors if without this
        )

        x = self.drop(output)
        x = self.fc(x)
        return x

for fold in range(skf.n_splits):
    print(f'-----------Fold: {fold+1} ------------------')
    train_loader, valid_loader = prepare_loaders(meld_train_df, fold=fold,tokenizer=ENtokenizer)
    ENmodel = ENSentimentClassifier(n_classes=7,freeze_bert=False).to(device)
    criterion = nn.CrossEntropyLoss()
    # Recommendation by BERT: lr: 5e-5, 2e-5, 3e-5
    # Batchsize: 16, 32
    optimizer = AdamW(ENmodel.parameters(), lr=2e-5)
    
    lr_scheduler = get_linear_schedule_with_warmup(
                optimizer, 
                num_warmup_steps=0, 
                num_training_steps=len(train_loader)*EPOCHS
            )
    best_acc = 0
    for epoch in range(EPOCHS):
        print(f'Epoch {epoch+1}/{EPOCHS}')
        print('-'*30)

        train(ENmodel, criterion, optimizer, train_loader)
        val_acc = eval(ENmodel)

        if val_acc > best_acc:
            torch.save(ENmodel.state_dict(), f'bert_fold{fold+1}.pth')
            best_acc = val_acc

def ENtest(data_loader):
    models = []
    for fold in range(skf.n_splits):
        model = ENSentimentClassifier(n_classes=7)
        model.to(device)
        model.load_state_dict(torch.load(f'bert_fold{fold+1}.pth'))
        model.eval()
        models.append(model)

    texts = []
    ENpredicts = []
    predict_probs = []
    ENreal_values = []

    for data in data_loader:
        text = data['text']
        input_ids = data['input_ids'].to(device)
        attention_mask = data['attention_masks'].to(device)
        targets = data['targets'].to(device)

        total_outs = []
        for model in models:
            with torch.no_grad():
                outputs = model(
                    input_ids=input_ids,
                    attention_mask=attention_mask
                )
                total_outs.append(outputs)
        
        total_outs = torch.stack(total_outs)
        _, pred = torch.max(total_outs.mean(0), dim=1)
        texts.extend(text)
        ENpredicts.extend(pred)
        predict_probs.extend(total_outs.mean(0))
        ENreal_values.extend(targets)
    
    ENpredicts = torch.stack(ENpredicts).cpu()
    predict_probs = torch.stack(predict_probs).cpu()
    ENreal_values = torch.stack(ENreal_values).cpu()
    print(classification_report(ENreal_values, ENpredicts))
    return ENreal_values, ENpredicts

test_dataset = SentimentDataset(meld_test_df, ENtokenizer, max_len=120)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True, num_workers=2)
ENreal_values, ENpredicts = ENtest(test_loader)

class_names = ['Enjoyment', 'Disgust', 'Sadness', 'Anger', 'Surprise', 'Fear', 'Neutral']
sns.heatmap(confusion_matrix(real_values, predicts, normalize='true'), annot=True, xticklabels = class_names, yticklabels = class_names) #caption needed. Write it all down!

infer("I'm so happy to meet you",ENmodel, ENtokenizer)



